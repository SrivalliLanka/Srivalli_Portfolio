# Portfolio Configuration File
# Update this file to modify any content on your portfolio website

# Hero Section
hero:
  name: "SRIVALLI LANKA"
  tagline: "Data Science & Analytics | Machine Learning | AI | Data Engineering"
  buttons:
    - text: "View My Work"
      href: "#projects"
      type: "primary"
    - text: "View Resume"
      action: "resume"
      type: "secondary"
    - text: "Get In Touch"
      href: "#contact"
      type: "secondary"

# About Section
about:
  title: "About Me"
  profileImage: "My_image.jpg"
  profileImageAlt: "Srivalli Lanka"
  fallbackInitials: "SL"
  paragraphs:
    - "I'm a Master's student in Information Management at the University of Illinois Urbana-Champaign with a strong interest in using data and AI to solve real-world problems. I currently maintain a 4.0 GPA and enjoy working across analytics, machine learning, and data-driven storytelling."
    - "Before graduate school, I worked at Deloitte as a Network Engineer, where I gained hands-on experience with large-scale enterprise systems and learned how technology decisions play out in real operational environments. That experience shaped my interest in building data solutions that are both technically sound and practically useful."
    - "Today, I work as a Research Assistant at RailTEC, analyzing freight-train derailment risk using statistical modeling and text analysis, and as a Data Analyst for CHIME-in, where I study the relationship between extreme heat and public health outcomes using statewide climate and health data."
    - "I enjoy turning complex, messy data into clear insights, whether through modeling, visualization, or AI-driven applications, and I'm motivated by work that blends analytical rigor with real-world impact."

# Skills Section
skills:
  title: "Skills"
  categories:
    - category: "Core Programming & Data"
      skills:
        - "Python"
        - "SQL"
        - "R"
        - "Pandas"
        - "NumPy"
        - "scikit-learn"
        - "Git"
        - "Tableau"
        - "Data Preparation"
        - "Data Modeling"
        - "Data Visualization"
    - category: "Data Engineering & Cloud"
      skills:
        - "AWS (S3, Lambda, Glue, EC2, SageMaker, Athena)"
        - "Apache Airflow"
        - "ETL"
        - "REST APIs"
        - "Data Pipelines"
        - "Data Integration"
    - category: "Statistics & Machine Learning"
      skills:
        - "Regression (Poisson/NB, Linear, Logistic)"
        - "Random Forest"
        - "XGBoost"
        - "NLP"
        - "Exploratory Data Analysis"
    - category: "‚≠ê Applied AI & Agentic Systems"
      skills:
        - "LLM-based application development"
        - "Memory-augmented AI (short-term, long-term, temporal)"
        - "Agentic system design"
        - "LLM orchestration & tool calling"
    - category: "üõ°Ô∏è AI Safety & Responsible AI"
      skills:
        - "Prompt-level guardrails"
        - "Human-in-the-loop escalation"
        - "Emergency workflow design"
        - "Edge-case & failure-mode handling"
    - category: "üß© Systems & Integration"
      skills:
        - "End-to-end AI system design"
        - "Memory‚Äìreasoning‚ÄìUI separation"
        - "Database-backed AI state management"
        - "External service integration (alerts, notifications)"
        - "Dashboard design"
        - "Analytics storytelling"

# Projects Section
projects:
  title: "Projects"
  items:
    - id: 1
      title: "RailTEC ‚Äî Freight Rail Safety Research"
      shortDescription: "Ongoing applied research program focused on understanding freight rail safety through statistical modeling, exploratory data analysis, and text analytics."
      detailedDescription:
        - "RailTEC is an ongoing applied research program focused on understanding freight rail safety through statistical modeling, exploratory data analysis, and text analytics. The work emphasizes methodological rigor, interpretability, and responsible use of data rather than finalized results or deployed systems."
        - "The research is organized into multiple independent workstreams, each addressing a specific analytical question related to derailment frequency, data structure, or reporting consistency. All workstreams are currently in exploratory or research-development stages."
        - "üëâ <strong>Detailed explanations, workflows, and visuals for each workstream are presented in the accompanying slide deck, which documents the research structure, methods, and current status in a clear, visual format.</strong>"
        - "<strong>Research Scope at a Glance:</strong> The RailTEC research program currently includes derailment rate modeling using Poisson and Negative Binomial regression, exposure and segment-level model design for rate interpretation, conceptual exploration of zero-heavy count models, NLP-based consistency analysis of accident narratives, and literature review on derailment severity modeling. Each workstream is treated as a standalone research inquiry, rather than as a single combined project."
      tech:
        - "Python"
        - "R"
        - "Statistical Modeling"
        - "Text Mining"
        - "NLP"
        - "Poisson Regression"
        - "Negative Binomial"
        - "LDA"
        - "Data Analysis"
      features:
        - "Derailment rate modeling using Poisson and Negative Binomial regression"
        - "Exposure and segment-level model design for rate interpretation"
        - "Conceptual exploration of zero-heavy count models (ZIP, hurdle models)"
        - "NLP-based consistency analysis of accident narratives"
        - "Topic modeling (LDA) for exploratory structure discovery"
        - "Segment-level data aggregation and exploratory data analysis"
        - "Working with sparse, heterogeneous, real-world safety data"
        - "Text preprocessing and tokenization for noisy narrative data"
        - "Keyword-based feature design using domain knowledge"
        - "Comparative analysis of structured vs unstructured data sources"
        - "Framing applied research questions under uncertainty"
        - "Human-in-the-loop validation and interpretive review"
      date: "May 2025 ‚Äì Present"
      github: ""
      live: ""
      thumbnail: "projects/RailTEC.png"
      heroMedia:
        type: "slides"
        url: "https://docs.google.com/presentation/d/e/2PACX-1vRvpRvUSp-CxnNVSfHDHOYmgF8kats9UboZTKnDyK_TOtAd5RFCu8fA7sXiqImVwTyYqB1Hx_cjyD8f/pubembed?start=true&loop=true&delayms=3000"
      media: []
    - id: 2
      title: "Carely ‚Äî A Memory-Augmented AI Companion"
      shortDescription: "An AI companion designed to support seniors with daily routines, emotional well-being, and memory, while prioritizing safety, trust, and human-centered design."
      detailedDescription:
        - "Carely is not just a chatbot. It is an AI companion designed to support seniors with daily routines, emotional well-being, and memory ‚Äî while prioritizing safety, trust, and human-centered design. The core challenge wasn't what Carely should say, but how an AI system should remember, reason, and respond responsibly over time."
        - "üß† <strong>Memory as a First-Class System:</strong> Carely is built with a layered memory architecture that explicitly separates short-term conversational context, long-term personal memory (people, routines, preferences), and temporal grounding (when information is valid). This allows Carely to recall meaningful details, maintain continuity across conversations, and reduce repetition ‚Äî while minimizing hallucinations. Memory storage and retrieval are explicit, auditable, and designed for safe scaling."
        - "üõ°Ô∏è <strong>Safety & Guardrails by Design:</strong> Carely embeds guardrails at the system level, not as an afterthought. This includes prompt-level safety constraints and intent detection, emergency escalation workflows that notify caregivers, and validation for ambiguous, emotional, or edge-case inputs. The system is intentionally designed to support‚Äînot replace‚Äîhuman caregivers, reinforcing trust and accountability."
        - "üó£Ô∏è <strong>Human-Centered Interaction:</strong> Carely focuses on reducing cognitive load through voice-first and chat-based interactions, natural language task completion (mood check-ins, medication logging), and an empathetic, non-robotic conversational tone refined through iteration. Users consistently described Carely as feeling more like a caring companion than a tool."
        - "üß™ <strong>Grounded Evaluation, Not Just Demos:</strong> Carely was evaluated with real users, including seniors and caregivers, across memory recall and continuity, emotional support and reminders, and emergency alert workflows. Usability metrics (UMUX-Lite) and qualitative feedback directly informed UI, memory, and safety improvements."
        - "‚ú® <strong>Why Carely Matters:</strong> Carely reflects an approach to AI that prioritizes architecture over prompts, safety over novelty, and humans over automation ‚Äî demonstrating how agentic systems can be built responsibly for real-world use."
      tech:
        - "Python"
        - "ChromaDB"
        - "SQLite"
        - "Generative AI"
        - "LLM"
        - "Vector Databases"
        - "Memory Architecture"
      features:
        - "Multi-layer memory architecture (short-term context + long-term personal memory + temporal grounding)"
        - "System-level safety guardrails and intent detection"
        - "Emergency escalation workflows with caregiver notifications"
        - "Voice-first and chat-based interactions"
        - "Natural language task completion (mood check-ins, medication logging)"
        - "Empathetic, human-centered conversational design"
        - "Real-user evaluation with seniors and caregivers"
        - "Usability metrics (UMUX-Lite) and iterative refinement"
        - "Explicit, auditable memory storage and retrieval"
        - "Support for‚Äînot replacement of‚Äîhuman caregivers"
      date: "Aug 2025 ‚Äì Dec 2025"
      github: ""
      live: ""
      thumbnail: "projects/Carely_logo.png"
      heroMedia:
        type: "video"
        url: "/projects/Carely.mp4"
      media: []
    - id: 3
      title: "Foundational Data Engineering Projects"
      shortDescription: "A collection of foundational data engineering projects demonstrating ETL pipelines, AWS cloud services, and data processing workflows."
      detailedDescription:
        - "This collection showcases foundational data engineering projects that demonstrate core ETL pipeline development, AWS cloud services integration, and data processing workflows. These projects highlight practical experience with real-world data engineering challenges."
      tech:
        - "Python"
        - "Apache Airflow"
        - "AWS S3"
        - "AWS EC2"
        - "AWS Glue"
        - "AWS Lambda"
        - "AWS Athena"
        - "QuickSight"
        - "ETL"
        - "Data Engineering"
      features:
        - "Cloud-based ETL pipeline development"
        - "AWS-native data processing architectures"
        - "Automated data ingestion and transformation"
        - "Interactive analytics dashboards"
        - "Serverless data processing workflows"
      date: "May 2025 ‚Äì Aug 2025"
      github: ""
      live: ""
      thumbnail: "projects/DE_logo.png"
      heroMedia:
        type: "image"
        url: "projects/DE_logo.png"
      media: []
      subProjects:
        - id: 3.1
          title: "Automated Weather Data ETL Pipeline"
          shortDescription: "Automated daily ingestion from OpenWeatherMap API ‚Üí cleaned dataset ‚Üí stored in S3 using an Airflow DAG."
          detailedDescription:
            - "This project was built as a hands-on exploration to understand how production-style ETL pipelines are designed, orchestrated, and deployed in a cloud environment using industry-standard tools."
            - ""
            - "<strong>Learning Objectives</strong>"
            - ""
            - "‚Ä¢ Understand REST API‚Äìbased data ingestion patterns"
            - "‚Ä¢ Gain hands-on experience with Apache Airflow DAGs and scheduling"
            - "‚Ä¢ Learn how ETL pipelines are deployed and monitored on AWS"
            - "‚Ä¢ Practice storing structured outputs in cloud object storage (S3)"
            - ""
            - '<div class="my-6"><img src="projects/Airflow_img.png" alt="Airflow" class="w-full max-w-4xl mx-auto rounded-lg shadow-lg" /></div>'
            - ""
            - "<strong>What I Built</strong>"
            - ""
            - "I implemented a cloud-based ETL pipeline that ingests live weather data from the OpenWeatherMap REST API and stores analytics-ready outputs in Amazon S3. The workflow is orchestrated using Apache Airflow running on an AWS EC2 instance, enabling scheduled execution, retries, and monitoring through the Airflow UI."
            - ""
            - "<strong>Architecture Overview</strong>"
            - ""
            - "‚Ä¢ <strong>Extract:</strong> Fetch weather data from the OpenWeatherMap API (JSON)"
            - "‚Ä¢ <strong>Transform:</strong> Normalize key attributes (timestamp, temperature, humidity, conditions)"
            - "‚Ä¢ <strong>Load:</strong> Persist structured outputs to Amazon S3"
            - "‚Ä¢ <strong>Orchestration:</strong> Airflow DAG manages scheduling, dependencies, retries, and logs"
            - "‚Ä¢ <strong>Deployment:</strong> Airflow hosted on AWS EC2 for continuous execution"
            - ""
            - "<strong>Reflection & Next Steps</strong>"
            - ""
            - "This project helped me understand how simple data pipelines are operationalized in real-world environments. As next steps, I would:"
            - ""
            - "‚Ä¢ Store outputs in Parquet format with date-based partitioning"
            - "‚Ä¢ Add data quality validation steps"
            - "‚Ä¢ Migrate orchestration to AWS MWAA or a Dockerized Airflow setup"
          tech:
            - "Python"
            - "Apache Airflow"
            - "AWS EC2"
            - "ETL"
            - "REST APIs"
          features:
            - "Real-time weather data ingestion"
            - "Fault-tolerant orchestration"
            - "Retry logic implementation"
            - "Python DAGs for workflow management"
            - "Integration with OpenWeatherMap API"
          date: "May 2025 ‚Äì Aug 2025"
          github: ""
          live: ""
          thumbnail: ""
          heroMedia: {}
          media: []
        - id: 3.2
          title: "YouTube Analytics Data Engineering Pipeline"
          shortDescription: "A serverless AWS data lake pipeline was built to ingest, process, query, and visualize YouTube trending data across multiple regions."
          detailedDescription:
            - "This project was completed as a hands-on data engineering exercise to gain practical experience building an end-to-end analytics pipeline on AWS using a real-world public dataset (YouTube trending data)."
            - ""
            - "The focus was on learning and implementing industry-standard tools and patterns."
            - ""
            - '<div class="my-6"><img src="projects/youtube-DE.jpg" alt="Flow" class="w-full max-w-4xl mx-auto rounded-lg shadow-lg" /><p class="text-center text-gray-600 font-medium mt-2">Flow</p></div>'
            - ""
            - "<strong>Data Ingestion</strong>"
            - ""
            - "‚Ä¢ Landed raw regional CSVs and category JSON files into an S3-based data lake"
            - "‚Ä¢ Preserved raw data using a schema-on-read approach"
            - ""
            - "<strong>Data Transformation</strong>"
            - ""
            - "‚Ä¢ Used Glue PySpark jobs to clean schemas and join datasets"
            - "‚Ä¢ Converted data into analytics-optimized Parquet format"
            - ""
            - "<strong>Analytics & Visualization</strong>"
            - ""
            - "‚Ä¢ Queried curated data directly from S3 using Athena"
            - "‚Ä¢ Built QuickSight dashboards to explore trends and engagement metrics"
            - ""
            - "<strong>Automation & Security</strong>"
            - ""
            - "‚Ä¢ Applied event-driven triggers using Lambda"
            - "‚Ä¢ Configured IAM roles following least-privilege access"
          tech:
            - "AWS S3"
            - "AWS Glue"
            - "AWS Lambda"
            - "AWS Athena"
            - "QuickSight"
            - "PySpark"
            - "AWS IAM"
            - "Data Engineering"
          features:
            - "AWS-native data pipeline architecture"
            - "YouTube metadata processing"
            - "Interactive KPI dashboards"
            - "Regional trend tracking"
            - "Serverless data processing"
          date: "May 2025 ‚Äì Aug 2025"
          github: ""
          live: ""
          thumbnail: ""
          heroMedia: {}
          media: []
    - id: 4
      title: "Entertainment Analytics Dashboard"
      shortDescription: "An end-to-end Tableau project integrating streaming, ratings, and awards data to explore audience preferences and industry recognition patterns."
      detailedDescription:
        - "This project was a hands-on analytics exploration focused on building a unified analytical view of the entertainment landscape. I integrated datasets from Netflix, IMDb, and the Academy Awards to design an interactive Tableau dashboard that enables data-driven exploration of content popularity, critical reception, and award recognition."
        - ""
        - "The emphasis was on data preparation, modeling, and visualization best practices commonly used in real-world analytics roles."
        - ""
        - "<strong>What I Did</strong>"
        - ""
        - "‚Ä¢ Integrated multiple heterogeneous datasets into a single, analysis-ready data model"
        - "‚Ä¢ Performed data cleaning, joins, filtering, and transformation to ensure consistency across sources"
        - "‚Ä¢ Designed interactive Tableau dashboards to support exploratory and comparative analysis"
        - "‚Ä¢ Translated raw data into clear visual insights aligned with business and strategic questions"
      tech:
        - "Tableau"
        - "Data Preparation"
        - "Data Modeling"
        - "Data Visualization"
        - "Data Integration"
        - "Analytical Thinking"
        - "Dashboard Design"
      features:
        - "Integrated multiple heterogeneous datasets into a single, analysis-ready data model"
        - "Performed data cleaning, joins, filtering, and transformation to ensure consistency across sources"
        - "Designed interactive Tableau dashboards to support exploratory and comparative analysis"
        - "Translated raw data into clear visual insights aligned with business and strategic questions"
        - "Interactive dashboards with calculated fields, filters, and parameters"
        - "Dataset harmonization, joins, cleaning, and validation"
        - "Heatmaps, scatter plots, bar charts, and geographic maps"
      date: "Dec 2024"
      github: ""
      live: ""
      thumbnail: "projects/tab1.png"
      heroMedia:
        type: "image"
        url: "projects/tab1.png"
      media: []

# Contact Section
contact:
  title: "Get In Touch"
  contactInfo:
    email:
      label: "Email"
      value: "srivallilanka@gmail.com"
      href: "mailto:srivallilanka@gmail.com"
    phone:
      label: "Phone"
      value: "+1 (331) 289-9444"
      href: "tel:+13312899444"
    linkedin:
      label: "LinkedIn"
      value: "linkedin.com/in/srivallilanka"
      href: "https://linkedin.com/in/srivallilanka"
  form:
    name:
      label: "Name"
      placeholder: "Your Name"
    email:
      label: "Email"
      placeholder: "your.email@example.com"
    message:
      label: "Message"
      placeholder: "Your message here..."
    submitButton: "Send Message"

# Navigation
navigation:
  brand: "Portfolio"
  brandHref: "#hero"
  menuItems:
    - text: "About"
      href: "#about"
    - text: "Projects"
      href: "#projects"
    - text: "Skills"
      href: "#skills"
    - text: "Contact"
      href: "#contact"

# Footer
footer:
  text: "¬© {year} Srivalli Lanka. All rights reserved."

# Resume
# To get your Google Drive file ID:
# 1. Upload your resume to Google Drive
# 2. Right-click the file and select "Get link" or "Share"
# 3. Make sure the file is set to "Anyone with the link can view"
# 4. Copy the file ID from the URL: https://drive.google.com/file/d/FILE_ID_HERE/view
# 5. Paste the FILE_ID_HERE below
resume:
  googleDriveFileId: "10pAh1Ofluv_ShMmJ78HIBERhjRTpzXDs"
  modalTitle: "Resume"
  viewButtonText: "View in Google Drive"
  downloadButtonText: "Download PDF"

